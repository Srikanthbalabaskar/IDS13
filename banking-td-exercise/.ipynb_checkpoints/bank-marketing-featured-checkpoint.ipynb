{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.features import rank2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_enable = True\n",
    "cv = 2\n",
    "verbose = 3\n",
    "number_of_jobs = -1\n",
    "rand_state = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf = pd.read_csv(\"featured_bank_imputed_wo_duration.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def festivals(month):\n",
    "    # imputing festivals - apr: easter; jun:carnival; dec:christmas\n",
    "    if month == 'apr' or month == 'jun' or month == 'dec':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.festivals'] = bdf['month'].apply(festivals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonus_months(month):\n",
    "    # bonus paid twice in a year - may and nov\n",
    "    if month == 'jun' or month == 'dec':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.bonus_months']= bdf['month'].apply(bonus_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commitment(housing, personal, marital):\n",
    "    # if a person is married and has housing and personal loans, flag it as high commitment\n",
    "    if housing == 'yes' and personal == 'yes' and marital == 'married':\n",
    "        return \"high\"\n",
    "    if housing == 'no' and personal == 'no' and marital in ['single']:\n",
    "        return \"low\"\n",
    "    else:\n",
    "        return \"medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.commitment'] = bdf.apply(lambda x: commitment(x.housing, x.loan, x.marital), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasons(month):\n",
    "    if month in ['mar','apr','may']:\n",
    "        return 'spring'\n",
    "    if month in ['jun','jul','aug']:\n",
    "        return 'summer'\n",
    "    if month in ['sep','oct','nov']:\n",
    "        return 'autumn'\n",
    "    if month in ['dec','jan','feb']:\n",
    "        return 'winter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.seasons'] = bdf['month'].apply(lambda mon: seasons(mon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasons_weightage(season):\n",
    "    if season == 'autumn':\n",
    "        return 0.21\n",
    "    if season == 'spring':\n",
    "        return 0.36\n",
    "    if season == 'summer':\n",
    "        return 0.40\n",
    "    if season == \"winter\":\n",
    "        return 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf ['f.season_weight'] = bdf['f.seasons'].apply(lambda season: seasons_weightage(season))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retired(age):\n",
    "    if age >= 65:\n",
    "        return 'retired'\n",
    "    else:\n",
    "        return 'not-retired'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.retired_status'] = bdf['age'].apply(lambda age: retired(age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marital_weightage(marital):\n",
    "    if marital == 'divorced':\n",
    "        return 0.1\n",
    "    if marital == 'single':\n",
    "        return 0.35\n",
    "    if marital == 'married':\n",
    "        return 0.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.marital_weightage'] = bdf['marital'].apply(lambda status: marital_weightage(status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_weightage(age):\n",
    "    if age == '11_to_20':\n",
    "        return 0.01\n",
    "    if age == '21_to_30':\n",
    "        return 0.23\n",
    "    if age == '31_to_40':\n",
    "        return 0.34\n",
    "    if age == '41_to_50':\n",
    "        return 0.18\n",
    "    if age == '51_to_60':\n",
    "        return 0.14\n",
    "    if age == '61_to_70':\n",
    "        return 0.04\n",
    "    if age == '71_to_80':\n",
    "        return 0.03\n",
    "    if age == '81_to_90':\n",
    "        return 0.01\n",
    "    if age == '91_to_100':\n",
    "        return 0.00\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.age_weightage'] = bdf['f.age'].apply(lambda age: age_weightage(age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_salary(job):\n",
    "    if job in ['blue-collar','housemaid']:\n",
    "        return 700*12\n",
    "    if job in ['admin.','technician']:\n",
    "        return 1200*12\n",
    "    if job in ['services']:\n",
    "        return 1000*12\n",
    "    if job in ['retired']:\n",
    "        return 700*12\n",
    "    if job in ['student', 'unemployed']:\n",
    "        return 600*12\n",
    "    if job in ['self-employed']:\n",
    "        return 1200*12\n",
    "    if job in ['entrepreneur']:\n",
    "        return 1500*12\n",
    "    if job in ['management']:\n",
    "        return 2200*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.salary'] = bdf['job'].apply(lambda job: job_salary(job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_tax(salary):\n",
    "    if salary >=0 and salary <=7112:\n",
    "        return 14.5\n",
    "    if salary >=7113 and salary <=10732:\n",
    "        return 23\n",
    "    if salary >=10733 and salary <=20322:\n",
    "        return 28.5\n",
    "    if salary >=20323 and salary <=25075:\n",
    "        return 35\n",
    "    if salary >=25076 and salary <=36967:\n",
    "        return 37\n",
    "    if salary >=36968:\n",
    "        return 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.income_tax'] = bdf['f.salary'].apply(lambda salary: income_tax(salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_user(previous):\n",
    "    if previous == 0:\n",
    "        return 'new user'\n",
    "    else:\n",
    "        return 'existing user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.user_type'] = bdf['previous'].apply(lambda previous: previous_user(previous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_no_of_times(previous):\n",
    "    # if a user is part of the previous campaign, bin them\n",
    "    if previous == 0:\n",
    "        return 'no previous'\n",
    "    if previous >= 1 and previous <= 3:\n",
    "        return '1_to_3'\n",
    "    if previous > 3:\n",
    "        return 'gt_3'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.previous_campaigns'] = bdf['previous'].apply(lambda previous: previous_no_of_times(previous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_campaign_calls(calls):\n",
    "    # how many times a user is called\n",
    "    if calls == 1:\n",
    "        return 'once'\n",
    "    if calls >= 2 and calls <= 3:\n",
    "        return 'twice to thrice '\n",
    "    if calls > 3 and calls <= 6:\n",
    "        return 'four to six times'\n",
    "    if calls >= 7 and calls <= 10:\n",
    "        return 'seven to ten times'\n",
    "    if calls > 10:\n",
    "        return 'more than ten times'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.current_campaign_calls'] = bdf['campaign'].apply(lambda calls: current_campaign_calls(calls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_time_user_calls(previous, campaigns):\n",
    "    # first time user - not a part of previous campaign\n",
    "    # first time the user is speaking - what's the conversion rate\n",
    "    if previous == 0:\n",
    "        if campaigns == 1:\n",
    "            return 'first time called'\n",
    "        if campaigns >= 2 and campaigns <= 3:\n",
    "            return 'called atleast twice'\n",
    "        if campaigns > 3:\n",
    "            return 'called atleast thrice'\n",
    "        else:\n",
    "            return 'more than thrice'\n",
    "    else:\n",
    "        return 'returning user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.first_time_user_calls'] = bdf.apply(lambda x: first_time_user_calls(x.previous, x.campaign), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savings_intention(job):\n",
    "    if job in ['admin.','blue-collar','technician']:\n",
    "        return 'high'\n",
    "    elif job in ['retired','management','services']:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.savings_intent_factor'] = bdf['job'].apply(lambda job: savings_intention(job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_range_weightage(age):\n",
    "    if age <= 24:\n",
    "        return 0.15\n",
    "    if age >=25 and age <= 69:\n",
    "        return 0.61\n",
    "    if age >= 70 and age <= 80:\n",
    "        return 0.13\n",
    "    if age > 80:\n",
    "        return 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.age_range_weightage'] = bdf['age'].apply(lambda age: age_range_weightage(age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_income_distribution(salary, marital):\n",
    "    if marital == 'married':\n",
    "        return round((salary/3),2)\n",
    "    if marital == 'single':\n",
    "        return salary\n",
    "    if marital == 'divorced':\n",
    "        return round((salary/2),2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.avg_income_dist'] = bdf.apply(lambda x: avg_income_distribution(x['f.salary'], x['marital']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_days_bin(pdays):\n",
    "    if pdays >=0 and pdays <=6:\n",
    "        return 'within a week'\n",
    "    if pdays >=7 and pdays <=13:\n",
    "        return 'within two weeks'\n",
    "    if pdays >= 14 and pdays <=20:\n",
    "        return 'within three weeks'\n",
    "    if pdays > 20:\n",
    "        return 'more than three weeks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf['f.pdays'] = bdf['pdays'].apply(lambda days: p_days_bin(days))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_columns= ['s.no', 'age', 'emp.var.rate','cons.conf.idx','nr.employed','default', 'pdays','f.salary','f.income_tax', 'euribor3m']\n",
    "categorical_columns=['job','education','marital','housing','loan','contact','month','day_of_week',\n",
    "                     'poutcome','f.euribor','f.age','f.commitment',\n",
    "                    'f.seasons','f.retired_status', 'f.user_type', 'f.pattern',\n",
    "                     'f.previous_campaigns','f.current_campaign_calls','f.first_time_user_calls', \n",
    "                     'f.pdays','f.savings_intent_factor'\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns\n",
    "bdf.drop(dropped_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert columns of object type to categorical columns\n",
    "bdf_cat = bdf[categorical_columns].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf[categorical_columns] = bdf[categorical_columns].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop bdf categorical columns from the dataframe\n",
    "bdf_noncat = bdf.drop(categorical_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['campaign', 'previous', 'cons.price.idx', 'y', 'f.festivals',\n",
       "       'f.bonus_months', 'f.season_weight', 'f.marital_weightage',\n",
       "       'f.age_weightage', 'f.savings_intent_factor', 'f.age_range_weightage',\n",
       "       'f.avg_income_dist'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdf_noncat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use one hot encoding for categorical columns\n",
    "bdf_cat_one_hot = pd.get_dummies(bdf_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41188, 97)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdf_cat_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat categorical df with non categorical df\n",
    "bdf_master = pd.concat([bdf_noncat, bdf_cat_one_hot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store only the target variable column\n",
    "y = bdf_master.pop('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the target variable column from the master dataset\n",
    "X = bdf_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     27416\n",
       "yes     3475\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     9132\n",
       "yes    1165\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the propotion of yes and no looks the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_classification_rpt(model):\n",
    "    visualizer = ClassificationReport(model, classes=['yes','no'], cmap=\"YlGn\", size=(600,300))\n",
    "    visualizer.fit(X_train, y_train)\n",
    "    visualizer.score(X_test, y_test)\n",
    "    visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that fits and predicts\n",
    "def fit_predict(algo,X_train, X_test, y_train, y_test):\n",
    "    algo.fit(X_train, y_train)\n",
    "    y_pred = algo.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Logistic Regression: "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'high'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-3f2846fac90f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy with Logistic Regression\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlogit_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-07649868fcb3>\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(algo, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# function that fits and predicts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0malgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[1;32m-> 1527\u001b[1;33m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[0;32m   1528\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    756\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'high'"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy with Logistic Regression\", end=': ')\n",
    "logit_accuracy = fit_predict(logit, X_train, X_test, y_train, y_test)\n",
    "print(logit_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_classification_rpt(logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy with Decision Tree\", end=': ')\n",
    "dt_accuracy = fit_predict(tree, X_train, X_test, y_train, y_test)\n",
    "print(dt_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_classification_rpt(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_dt(train, test, y_train, y_test, scaler, max_depth,\n",
    "               criterion = 'entropy', max_features=1, min_samples_split=4):\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "    test_scaled = scaler.transform(test)\n",
    "    dt = DecisionTreeClassifier(criterion = criterion, max_depth=max_depth,\n",
    "                               random_state= 101, max_features=max_features,\n",
    "                               min_samples_split=min_samples_split)\n",
    "    dt.fit(train_scaled, y_train)\n",
    "    y_pred = dt.predict(test_scaled)\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max depth parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_max_depth = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,30):\n",
    "    print(\"Accuracy score using max_depth = \", i, end = ':')\n",
    "    dt_max_depth = fit_predict_dt(X_train, X_test, y_train, y_test, StandardScaler(), i)\n",
    "    print(dt_max_depth)\n",
    "    list_max_depth.append(dt_max_depth)\n",
    "    \n",
    "max_depth_tuned = list_max_depth.index(max(list_max_depth))+1\n",
    "print(max_depth_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max features tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_max_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(0.1,1.0,0.1):\n",
    "    print('Accuracy score using max features =', i, end = \":\")\n",
    "    dt_max_features = fit_predict_dt(X_train, X_test, y_train, y_test, StandardScaler(), max_depth=max_depth_tuned, max_features=i)\n",
    "    print(dt_max_features)\n",
    "    dict_max_features[i]=dt_max_features\n",
    "\n",
    "max_feature_tuned = max(dict_max_features, key=dict_max_features.get)\n",
    "print(max_feature_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min samples split tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_min_samples_split = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,10):\n",
    "    print('Accuracy score using min samples split=', i, end=\":\")\n",
    "    dt_min_sample_split = fit_predict_dt(X_train, X_test, y_train, y_test, StandardScaler(), max_depth=max_depth_tuned, max_features=max_feature_tuned, min_samples_split=i)\n",
    "    print(dt_min_sample_split)\n",
    "    dict_min_samples_split[i] = dt_min_sample_split\n",
    "    \n",
    "min_sample_split_tuned = max(dict_min_samples_split, key=dict_min_samples_split.get)\n",
    "print(min_sample_split_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_index_tuned = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['gini','entropy']:\n",
    "    print(\"Accuracy score using criterion: \", i, end = ':')\n",
    "    dt_index_score = fit_predict_dt(X_train, X_test, y_train, y_test, StandardScaler(), max_depth=max_depth_tuned, max_features = max_feature_tuned, min_samples_split=min_sample_split_tuned, criterion= i)\n",
    "    print(dt_index_score)\n",
    "    dict_index_tuned[i] = dt_index_score\n",
    "\n",
    "dt_index_tuned = max(dict_index_tuned, key=dict_index_tuned.get)\n",
    "print(dt_index_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerunning decision tree with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best_param_score  =  fit_predict_dt(X_train, X_test, y_train, y_test, StandardScaler(), max_depth=max_depth_tuned, max_features = max_feature_tuned, min_samples_split=min_sample_split_tuned, criterion= dt_index_tuned)\n",
    "print(\"Accuracy score for decision tree using best param: \", end = ':')\n",
    "print(dt_best_param_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1 = DecisionTreeClassifier(criterion = dt_index_tuned, max_depth=max_depth_tuned,\n",
    "                               random_state= rand_state, max_features=max_feature_tuned,\n",
    "                               min_samples_split=min_sample_split_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_classification_rpt(tree1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poly(train, test, degree):\n",
    "    poly = PolynomialFeatures(degree = degree)\n",
    "    train_poly = poly.fit_transform(train)\n",
    "    test_poly = poly.fit_transform(test)\n",
    "    return train_poly, test_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## polynomial features taking a lot of time as the number of columns are more. Hence commented the following piece of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for degree in [1,2,3,4]:\n",
    "#     train_poly, test_poly = create_poly(X_train, X_test, degree)\n",
    "#     print(\"polynomial degree\", degree)\n",
    "#     fit_predict(train_poly, test_poly, y_train, y_test, StandardScaler(), 16, max_features = 0.2,min_samples_split=4, criterion='entropy')\n",
    "#     print(10 *'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(criterion='entropy', oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy with Random Forest\", end=': ')\n",
    "rf_accuracy = fit_predict(forest, X_train, X_test, y_train, y_test)\n",
    "print(rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_classification_rpt(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': [200,500,700],\n",
    "    'max_depth': [10,15,20,25],\n",
    "    'min_samples_leaf': [3,5,7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(forest, params, cv=cv, verbose=verbose, n_jobs=number_of_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    forest_best_max_depth = gs.best_params_['max_depth']\n",
    "    forest_best_min_samples = gs.best_params_['min_samples_leaf']\n",
    "    forest_best_n_estimators = gs.best_params_['n_estimators']\n",
    "    #forest_best_criterion = gs.best_params_['criterion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    print(gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining with best estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    forest1 = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='entropy', max_depth=forest_best_max_depth, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=forest_best_min_samples, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=forest_best_n_estimators,\n",
    "                       n_jobs=number_of_jobs, oob_score=True, random_state=rand_state,\n",
    "                       verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    forest1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    pred_forest1 = forest1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    print(\"Accuracy with Random Forest after Hyper Parameter Tuning\", end=': ')\n",
    "    rf_hyper_accuracy = accuracy_score(y_test, pred_forest1)\n",
    "    print(rf_hyper_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    for x in sorted(list(zip(forest1.feature_importances_, X_train.columns)), reverse=True):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    print(confusion_matrix(y_test, pred_forest1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Summary Without Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy with Logistic Regression\", end=': ')\n",
    "print(logit_accuracy)\n",
    "print(\"Accuracy with Decision Tree\", end=': ')\n",
    "print(dt_accuracy)\n",
    "print(\"Accuracy of Decision Tree after Hyper Parameter Tuning: \", end = ':')\n",
    "print(dt_best_param_score)\n",
    "print(\"Accuracy with Random Forest\", end=': ')\n",
    "print(rf_accuracy)\n",
    "if grid_search_enable == True:\n",
    "    print(\"Accuracy with Random Forest after Hyper Parameter Tuning\", end=': ')\n",
    "    print(rf_hyper_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adabst_fit = AdaBoostClassifier(base_estimator=logit, random_state=rand_state, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adabst_fit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy with Ada Boost\", end=': ')\n",
    "ada_boost_predict = adabst_fit.predict(X_test)\n",
    "ada_boost_accuracy = accuracy_score(y_test,ada_boost_predict)\n",
    "print(ada_boost_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, adabst_fit.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_adaboost = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'base_estimator': [logit, tree1],\n",
    "    'learning_rate': [0.5,0.75,1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_gs = GridSearchCV(adabst_fit, params_adaboost, cv=cv, n_jobs=number_of_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    ada_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    adabst_best_estimator = ada_gs.best_params_['base_estimator']\n",
    "    adabst_best_learning_rate = ada_gs.best_params_['learning_rate']\n",
    "    adabst_best_n_estimators = ada_gs.best_params_['n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    ada_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost - Retraining with best estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    ada_best = AdaBoostClassifier(base_estimator=adabst_best_estimator, random_state=rand_state,\n",
    "                                  n_estimators=adabst_best_n_estimators, learning_rate=adabst_best_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    ada_best.fit(X_train, y_train)\n",
    "    ada_best_predict = ada_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    print(\"Accuracy with Adaboost after Hyper Parameter Tuning\", end=': ')\n",
    "    ada_best_hyper_accuracy = accuracy_score(y_test, ada_best_predict)\n",
    "    print(ada_best_hyper_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_fit = GradientBoostingClassifier(n_estimators=500,\n",
    "                                     min_samples_split=2,min_samples_leaf=1,max_depth=1,random_state=rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_fit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy with Gradient Boost\", end=': ')\n",
    "gbc_accuracy = accuracy_score(y_test,gbc_fit.predict(X_test))\n",
    "print(gbc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boost with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gbc = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [2,3,4],\n",
    "    'learning_rate': [0.05,0.075,0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_gs = GridSearchCV(gbc_fit, params_gbc, cv=cv, n_jobs=number_of_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    gbc_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    gbc_best_estimator = gbc_gs.best_params_['n_estimators']\n",
    "    gbc_best_max_depth = gbc_gs.best_params_['max_depth']\n",
    "    gbc_best_learning_rate = gbc_gs.best_params_['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    print(gbc_gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost - With Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    gbc_best = GradientBoostingClassifier(n_estimators=gbc_best_estimator,\n",
    "                                     min_samples_split=2,min_samples_leaf=1,max_depth=gbc_best_max_depth,random_state=rand_state,\n",
    "                                         learning_rate=gbc_best_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    gbc_best.fit(X_train, y_train)\n",
    "    gbc_best_predict = gbc_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    print(\"Accuracy with Gradient Boost after Hyper Parameter Tuning\", end=': ')\n",
    "    gbc_best_hyper_accuracy = accuracy_score(y_test, gbc_best_predict)\n",
    "    print(gbc_best_hyper_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_fit = XGBClassifier(max_depth=2, n_estimators=5000, random_state=rand_state, n_jobs=number_of_jobs)\n",
    "xgb_fit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy with XG Boost\", end=': ')\n",
    "xgb_predict = xgb_fit.predict(X_test)\n",
    "xgb_accuracy = accuracy_score(y_test,xgb_predict)\n",
    "print(xgb_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'max_depth': [2,3,4],\n",
    "    'n_estimators': [300,500,700],\n",
    "    'learning_rate': [0.05,0.075,0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs = GridSearchCV(xgb_fit, xgb_params, cv=cv, n_jobs=number_of_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    xgb_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    print(xgb_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    xgb_best_max_depth = xgb_gs.best_params_['max_depth']\n",
    "    xgb_best_n_estimators = xgb_gs.best_params_['n_estimators']\n",
    "    xgb_best_learning_rate = xgb_gs.best_params_['learning_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-running XG Boost with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    xgb_best = XGBClassifier(max_depth=xgb_best_max_depth,\n",
    "                             n_estimators=xgb_best_n_estimators, random_state=rand_state,\n",
    "                             n_jobs=number_of_jobs, learning_rate = xgb_best_learning_rate)\n",
    "    xgb_best.fit(X_train, y_train)\n",
    "    xgb_best_predict = xgb_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_enable == True:\n",
    "    print(\"Accuracy with XG Boost after Hyper Parameter Tuning\", end=': ')\n",
    "    xgb_best_hyper_accuracy = accuracy_score(y_test, xgb_best_predict)\n",
    "    print(xgb_best_hyper_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Summary With Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy with Logistic Regression\", end=': ')\n",
    "print(logit_accuracy)\n",
    "\n",
    "print(\"Accuracy with Decision Tree\", end=': ')\n",
    "print(dt_accuracy)\n",
    "print(\"Accuracy with Decision Tree after Hyper Parameter Tuning: \", end = ':')\n",
    "print(dt_best_param_score)\n",
    "\n",
    "print(\"Accuracy with Random Forest\", end=': ')\n",
    "print(rf_accuracy)\n",
    "if grid_search_enable == True:\n",
    "    print(\"Accuracy with Random Forest after Hyper Parameter Tuning\", end=': ')\n",
    "    print(rf_hyper_accuracy)\n",
    "\n",
    "print(\"Accuracy with Ada Boost\", end=': ')\n",
    "print(ada_boost_accuracy)\n",
    "if grid_search_enable == True:\n",
    "    print(\"Accuracy with Ada Boost after Hyper Parameter Tuning\", end=': ')\n",
    "    print(ada_best_hyper_accuracy)\n",
    "\n",
    "print(\"Accuracy with Gradient Boost\", end=': ')\n",
    "print(gbc_accuracy)\n",
    "if grid_search_enable == True:\n",
    "    print(\"Accuracy with Gradient Boost after Hyper Parameter Tuning\", end=': ')\n",
    "    print(gbc_best_hyper_accuracy)\n",
    "    \n",
    "print(\"Accuracy with XG Boost\", end=': ')\n",
    "print(xgb_accuracy)\n",
    "if grid_search_enable == True:\n",
    "    print(\"Accuracy with XG Boost after Hyper Parameter Tuning\", end=': ')\n",
    "    print(xgb_best_hyper_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
